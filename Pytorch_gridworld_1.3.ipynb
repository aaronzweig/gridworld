{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple\n",
    "from itertools import count\n",
    "from PIL import Image\n",
    "from pyemd import emd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "\n",
    "# set up matplotlib\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "# if gpu is to be used\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Saves a transition.\"\"\"\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.position] = Transition(*args)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "def combine(state, goal):\n",
    "    if state is None:\n",
    "        return None\n",
    "    return torch.cat((state, goal), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class miniDQN(nn.Module):\n",
    "\n",
    "    def __init__(self, hidden):\n",
    "        super(miniDQN, self).__init__()\n",
    "        self.W1 = nn.Linear(8, hidden)\n",
    "        self.W2 = nn.Linear(hidden, 4)\n",
    "        nn.init.xavier_uniform_(self.W1.weight)\n",
    "        nn.init.xavier_uniform_(self.W2.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.W1(x))\n",
    "        return self.W2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_row = 8\n",
    "num_col = 8\n",
    "\n",
    "action_direction = [(0,1), (0,-1), (1,0), (-1,0)]\n",
    "block = { \"empty\" : 0, \"blue\" : 1, \"red\" : 2, \"goal\" : 3, \"wall\" : 4}\n",
    "\n",
    "reds = [(4,0)]\n",
    "blues = [(0,4)]\n",
    "start = (0,0)\n",
    "end = (6,6)\n",
    "walls = []\n",
    "\n",
    "start_state = np.asarray(list(start) + [0,0])\n",
    "end_state = np.asarray(list(end) + [1,1])\n",
    "\n",
    "\n",
    "def is_done(s):\n",
    "    row, col, blue, red = tuple(s)\n",
    "    return blue and red and (row, col) == end\n",
    "\n",
    "def step(s, a):\n",
    "    row, col, blue, red = tuple(s)\n",
    "    a_row, a_col = action_direction[a]\n",
    "\n",
    "    row = row + a_row\n",
    "    col = col + a_col\n",
    "\n",
    "    if row < 0 or row >= num_row or col < 0 or col >= num_col or (row, col) in walls:\n",
    "        return s\n",
    "\n",
    "    blue = blue or (row, col) in blues\n",
    "    red = red or (row, col) in reds\n",
    "\n",
    "    return np.asarray((row, col, blue, red))\n",
    "\n",
    "def get_reward(s, a, s_prime):\n",
    "    if is_done(s):\n",
    "        return 0.0\n",
    "    return -1.0\n",
    "\n",
    "def L1(s1, s2):\n",
    "    return 1.0 * np.sum(np.abs(np.asarray(s1) - np.asarray(s2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Very adhoc, requires no dynamics\n",
    "distance_cache = {}\n",
    "\n",
    "def distance(a, b):\n",
    "    global distance_cache\n",
    "    ta = tuple(a)\n",
    "    tb = tuple(b)\n",
    "    if (ta,tb) in distance_cache:\n",
    "        return distance_cache[(ta,tb)]\n",
    "    \n",
    "    row_a, col_a, blue_a, red_a = ta\n",
    "    row_b, col_b, blue_b, red_b = tb\n",
    "    \n",
    "    blue_match = blue_a == blue_b\n",
    "    red_match = red_a == red_b\n",
    "    \n",
    "    if red_match and not blue_match:\n",
    "        dist = min([L1((row_a,col_a),blue) + L1(blue, (row_b, col_b)) for blue in blues])\n",
    "    elif not red_match and blue_match:\n",
    "        dist = min([L1((row_a,col_a),red) + L1(red, (row_b, col_b)) for red in reds])\n",
    "    elif not red_match and not blue_match:\n",
    "        x = min([L1((row_a,col_a),red) + L1(red, blue) + L1(blue, (row_b, col_b)) for red, blue in zip(reds, blues)])\n",
    "        y = min([L1((row_a,col_a),blue) + L1(blue, red) + L1(red, (row_b, col_b)) for red, blue in zip(reds, blues)])\n",
    "        dist = min(x,y)\n",
    "    else:\n",
    "        dist = L1(ta, tb)\n",
    "    distance_cache[(ta,tb)] = dist\n",
    "    return dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "GAMMA = 0.999\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 4000\n",
    "TARGET_UPDATE = 50\n",
    "HIDDEN_LAYER_SIZE = 30\n",
    "EPISODE_LENGTH = 75\n",
    "LEARNING_RATE = 0.0008\n",
    "\n",
    "DIVERSE_EXPERT = True\n",
    "BACKPLAY = True\n",
    "\n",
    "num_episodes = 40000\n",
    "random.seed(6)\n",
    "distance_cache = {}\n",
    "\n",
    "policy_net = miniDQN(HIDDEN_LAYER_SIZE).to(device)\n",
    "target_net = miniDQN(HIDDEN_LAYER_SIZE).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "\n",
    "optimizer = optim.Adam(policy_net.parameters(), lr = LEARNING_RATE)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, 0.99999)\n",
    "memory = ReplayMemory(40000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_done = 0\n",
    "\n",
    "def select_action(state, goal, test = False):\n",
    "    global steps_done\n",
    "    sample = random.random()\n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * math.exp(-1. * steps_done / EPS_DECAY)\n",
    "    steps_done += 1\n",
    "    if test or sample > eps_threshold:\n",
    "        with torch.no_grad():\n",
    "            x = torch.cat((state, goal), 1)\n",
    "            return policy_net(x).max(1)[1].view(1, 1)\n",
    "    else:\n",
    "        return torch.tensor([[random.randrange(4)]], device=device, dtype=torch.long)\n",
    "\n",
    "def select_action_entropy(state, goal, alpha):\n",
    "    x = torch.cat((state, goal), 1)\n",
    "    potential = (alpha * policy_net(x).detach()).exp().squeeze()\n",
    "    dist = (potential / potential.sum()).numpy()\n",
    "    return torch.tensor([[np.random.choice(4, p = dist)]], device=device, dtype=torch.long)\n",
    "    \n",
    "\n",
    "def select_expert_action(state, goal):\n",
    "    row, col, blue, red = tuple(state)\n",
    "    action_scores = np.zeros(4)\n",
    "    for a in range(4):\n",
    "        #Assumes determinism\n",
    "        next_state = step(state, a)\n",
    "        action_scores[a] = distance(next_state, goal)\n",
    "    action = np.random.choice(np.flatnonzero(action_scores == action_scores.min()))\n",
    "    return torch.tensor([[action]], device=device, dtype=torch.long)    \n",
    "\n",
    "def sample_expert_trajectory():\n",
    "    state = start_state\n",
    "    next_state = None\n",
    "\n",
    "    trajectory = []\n",
    "    \n",
    "    for t in range(EPISODE_LENGTH):\n",
    "        action = select_expert_action(state, end_state)\n",
    "        \n",
    "        torch_state = torch.from_numpy(state).float().unsqueeze(0)\n",
    "        next_state = step(state, action.item())\n",
    "        torch_next_state = torch.from_numpy(next_state).float().unsqueeze(0)\n",
    "        reward = get_reward(state, action.item(), next_state)\n",
    "        done = is_done(state)\n",
    "        reward = torch.tensor([reward], device=device)\n",
    "        if done:\n",
    "            torch_next_state = None\n",
    "\n",
    "        trajectory.append((torch_state, action, torch_next_state, reward))\n",
    "        if done:\n",
    "            break\n",
    "        state = next_state\n",
    "    return trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model():\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,batch.next_state)),\n",
    "                                  device=device, dtype=torch.uint8)\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state if s is not None])\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
    "\n",
    "    #DDQN\n",
    "    policy_action_indices = policy_net(non_final_next_states).max(1)[1].detach()\n",
    "    target_actions = target_net(non_final_next_states).detach()\n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "    next_state_values[non_final_mask] = target_actions.gather(1, policy_action_indices.unsqueeze(1)).squeeze()\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "    loss = F.smooth_l1_loss(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "    \n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    for param in policy_net.parameters():\n",
    "        param.grad.data.clamp_(-1, 1)\n",
    "    optimizer.step()\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "2.1680260206523694\n",
      "1000\n",
      "0.04666393666250584\n",
      "2000\n",
      "0.12309491512466351\n",
      "3000\n",
      "0.3514441646369647\n",
      "4000\n",
      "0.27041079194123224\n",
      "5000\n",
      "0.26135663845699747\n",
      "6000\n",
      "0.13802155052809245\n",
      "7000\n",
      "0.19490906885353051\n",
      "8000\n",
      "0.38986544502118736\n",
      "9000\n",
      "0.4837424426547403\n",
      "10000\n",
      "0.3542641099774379\n",
      "11000\n",
      "0.1161667428795856\n",
      "12000\n",
      "0.08058793691787965\n",
      "13000\n",
      "0.07076517516224032\n",
      "14000\n",
      "0.03808578766909613\n",
      "15000\n",
      "0.007336091710721726\n",
      "16000\n",
      "0.00420067164045383\n",
      "17000\n",
      "0.008227111381766425\n",
      "18000\n",
      "0.010017994821338838\n",
      "19000\n",
      "0.007055548939348065\n",
      "20000\n",
      "0.00666453392841716\n",
      "21000\n",
      "0.008478398370269268\n",
      "22000\n",
      "0.004295759275985837\n",
      "23000\n",
      "0.0033461334071038656\n",
      "24000\n",
      "0.002422072687700963\n",
      "25000\n",
      "0.0029711568362704504\n",
      "26000\n",
      "0.01688124093129019\n",
      "27000\n",
      "0.0070400458811909995\n",
      "28000\n",
      "0.005273298605128459\n",
      "29000\n",
      "0.002779538278213819\n",
      "30000\n",
      "0.007325481924704427\n",
      "31000\n",
      "0.007920948982089506\n",
      "32000\n",
      "0.0068493822990630384\n",
      "33000\n",
      "0.0044673483301123544\n",
      "34000\n",
      "0.013496104122349044\n",
      "35000\n",
      "0.015354310896390553\n",
      "36000\n",
      "0.0050932630577498915\n",
      "37000\n",
      "0.002679690474500781\n",
      "38000\n",
      "0.00428678142758756\n",
      "39000\n",
      "0.003349144414116192\n",
      "[0. 0. 0. 0.]\n",
      "[0. 1. 0. 0.]\n",
      "[0. 2. 0. 0.]\n",
      "[0. 3. 0. 0.]\n",
      "[0. 4. 1. 0.]\n",
      "[1. 4. 1. 0.]\n",
      "[1. 3. 1. 0.]\n",
      "[1. 2. 1. 0.]\n",
      "[1. 1. 1. 0.]\n",
      "[1. 0. 1. 0.]\n",
      "[2. 0. 1. 0.]\n",
      "[3. 0. 1. 0.]\n",
      "[4. 0. 1. 1.]\n",
      "[5. 0. 1. 1.]\n",
      "[6. 0. 1. 1.]\n",
      "[6. 1. 1. 1.]\n",
      "[6. 2. 1. 1.]\n",
      "[6. 3. 1. 1.]\n",
      "[6. 4. 1. 1.]\n",
      "[6. 5. 1. 1.]\n",
      "[6. 6. 1. 1.]\n",
      "Complete\n"
     ]
    }
   ],
   "source": [
    "losses = [0]\n",
    "torch_end = torch.from_numpy(end_state).float().unsqueeze(0)\n",
    "expert_trajectories = [sample_expert_trajectory()]\n",
    "for i in range(4000):\n",
    "    if DIVERSE_EXPERT:\n",
    "        expert_trajectories.append(sample_expert_trajectory())\n",
    "    for torch_state, action, torch_next_state, reward in expert_trajectories[-1]:\n",
    "        memory.push(combine(torch_state, torch_end), action, combine(torch_next_state, torch_end), reward)\n",
    "\n",
    "for i_episode in range(num_episodes):\n",
    "    state = start_state\n",
    "    next_state = None\n",
    "    final = i_episode == num_episodes - 1\n",
    "    trajectory = []\n",
    "    \n",
    "    if i_episode % 500 == 0:\n",
    "        scheduler.step()\n",
    "    if BACKPLAY:\n",
    "        trajectory_index = np.random.choice(len(expert_trajectories))\n",
    "        expert_trajectory = expert_trajectories[trajectory_index]\n",
    "        \n",
    "        start_index = np.random.choice(len(expert_trajectory))\n",
    "        if np.random.random_sample() < 0.5 or final:\n",
    "            start_index = 0\n",
    "        state = expert_trajectory[start_index][0].squeeze().data.numpy()\n",
    "    \n",
    "    \n",
    "    \n",
    "    for t in range(EPISODE_LENGTH):\n",
    "        torch_state = torch.from_numpy(state).float().unsqueeze(0)\n",
    "\n",
    "        action = select_action(torch_state, torch_end, final)\n",
    "            \n",
    "        next_state = step(state, action.item())\n",
    "        torch_next_state = torch.from_numpy(next_state).float().unsqueeze(0)\n",
    "        reward = get_reward(state, action.item(), next_state)\n",
    "        done = is_done(state)\n",
    "        reward = torch.tensor([reward], device=device)\n",
    "\n",
    "        if done:\n",
    "            torch_next_state = None\n",
    "\n",
    "        trajectory.append((torch_state, action, torch_next_state, reward))\n",
    "\n",
    "        loss = optimize_model()\n",
    "        losses.append(loss if loss is not None else 0)\n",
    "        if final:\n",
    "            print(state)\n",
    "        if done:\n",
    "            break\n",
    "        state = next_state\n",
    "\n",
    "    if i_episode % 1000 == 0:\n",
    "        print(i_episode)\n",
    "        print(np.mean(np.asarray(losses)))\n",
    "        losses = []\n",
    "\n",
    "    #HER updates with goal as real goal OR last state in trajectory\n",
    "    for torch_state, action, torch_next_state, reward in trajectory:\n",
    "        memory.push(combine(torch_state, torch_end), action, combine(torch_next_state, torch_end), reward)\n",
    "        if not is_done(state):\n",
    "            torch_local_goal = torch.from_numpy(state).float().unsqueeze(0)\n",
    "            local_reward = torch.equal(torch_next_state, torch_local_goal)\n",
    "            new_reward = torch.add(reward, local_reward)\n",
    "            memory.push(combine(torch_state, torch_local_goal), action, combine(torch_next_state, torch_local_goal), new_reward)\n",
    "                    \n",
    "                          \n",
    "    # Update the target network\n",
    "    if i_episode % TARGET_UPDATE == 0:\n",
    "        target_net.load_state_dict(policy_net.state_dict())\n",
    "    \n",
    "        \n",
    "print('Complete')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{(0, 1, 0, 0): 0.9800000000000008, (0, 0, 0, 0): 0.020000000000000004}\n",
      "0.0020009859999993163\n"
     ]
    }
   ],
   "source": [
    "def predicted_distance(state, goal):\n",
    "    torch_state = torch.from_numpy(state).float().unsqueeze(0)\n",
    "    torch_goal = torch.from_numpy(goal).float().unsqueeze(0)\n",
    "    x = torch.cat((torch_state, torch_goal), 1)\n",
    "    return -1 * policy_net(x).max(1)[0].view(1, 1).item()\n",
    "\n",
    "def predicted_measure_ball(state, goal, n = 500):\n",
    "    measure = {}\n",
    "    torch_state = torch.from_numpy(state).float().unsqueeze(0)\n",
    "    torch_goal = torch.from_numpy(goal).float().unsqueeze(0)\n",
    "    x = torch.cat((torch_state, torch_goal), 1)\n",
    "    for i in range(n):\n",
    "        action = select_action_entropy(torch_state, torch_goal, alpha = 4.0)\n",
    "        next_state = step(state, action.item())\n",
    "        key = tuple(next_state)\n",
    "        measure[key] = measure.get(key, 0) + 1.0/n\n",
    "    return measure\n",
    "\n",
    "def wasserstein(a, b, goal):\n",
    "    if tuple(a) == tuple(b):\n",
    "        return 0\n",
    "    m_a = predicted_measure_ball(a, goal)\n",
    "    m_b = predicted_measure_ball(b, goal)\n",
    "    states = list(set(m_a.keys()) | set(m_b.keys()))\n",
    "    \n",
    "    m_a = [m_a.get(s, 0.0) for s in states]\n",
    "    m_b = [m_b.get(s, 0.0) for s in states]\n",
    "    dist = [[distance(np.asarray(s1),np.asarray(s2)) for s1 in states] for s2 in states]\n",
    "    return emd(np.asarray(m_a), np.asarray(m_b), np.asarray(dist))\n",
    "\n",
    "def curvature(a, b, goal):\n",
    "    return 1 - wasserstein(a, b, goal) / max(distance(a, b), 0.001)\n",
    "\n",
    "def forward_curvature(state, goal):\n",
    "    #Assumes deterministic dynamics!\n",
    "    next_states = [step(state, a) for a in range(4)]\n",
    "    return min([curvature(state, next_state, goal) for next_state in next_states])\n",
    "\n",
    "def print_space(fn, goal):\n",
    "    np.set_printoptions(precision=2, suppress=True)\n",
    "    for red in [0,1]:\n",
    "        for blue in [0,1]:\n",
    "            grid = np.zeros((num_row, num_col))\n",
    "            for row in range(num_row):\n",
    "                for col in range(num_col):\n",
    "                    state = np.asarray((row, col, blue, red))\n",
    "                    grid[row, col] = fn(state, goal)\n",
    "            print(\"red = \" + str(red))\n",
    "            print(\"blue = \" + str(blue))\n",
    "            print(grid)\n",
    "\n",
    "a = np.asarray([2,0,0,0])\n",
    "b = np.asarray([1,0,0,0])\n",
    "print(predicted_measure_ball(start_state, end_state))\n",
    "print(curvature(a,b,end_state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "red = 0\n",
      "blue = 0\n",
      "[[20. 19. 18. 17. 16. 17. 18. 19.]\n",
      " [19. 20. 19. 18. 17. 18. 19. 20.]\n",
      " [18. 19. 20. 19. 18. 19. 20. 21.]\n",
      " [17. 18. 19. 20. 19. 20. 21. 22.]\n",
      " [16. 17. 18. 19. 20. 21. 22. 23.]\n",
      " [17. 18. 19. 20. 21. 22. 23. 24.]\n",
      " [18. 19. 20. 21. 22. 23. 24. 25.]\n",
      " [19. 20. 21. 22. 23. 24. 25. 26.]]\n",
      "red = 0\n",
      "blue = 1\n",
      "[[12. 13. 14. 15. 16. 17. 18. 19.]\n",
      " [11. 12. 13. 14. 15. 16. 17. 18.]\n",
      " [10. 11. 12. 13. 14. 15. 16. 17.]\n",
      " [ 9. 10. 11. 12. 13. 14. 15. 16.]\n",
      " [ 8.  9. 10. 11. 12. 13. 14. 15.]\n",
      " [ 9. 10. 11. 12. 13. 14. 15. 16.]\n",
      " [10. 11. 12. 13. 14. 15. 16. 17.]\n",
      " [11. 12. 13. 14. 15. 16. 17. 18.]]\n",
      "red = 1\n",
      "blue = 0\n",
      "[[12. 11. 10.  9.  8.  9. 10. 11.]\n",
      " [13. 12. 11. 10.  9. 10. 11. 12.]\n",
      " [14. 13. 12. 11. 10. 11. 12. 13.]\n",
      " [15. 14. 13. 12. 11. 12. 13. 14.]\n",
      " [16. 15. 14. 13. 12. 13. 14. 15.]\n",
      " [17. 16. 15. 14. 13. 14. 15. 16.]\n",
      " [18. 17. 16. 15. 14. 15. 16. 17.]\n",
      " [19. 18. 17. 16. 15. 16. 17. 18.]]\n",
      "red = 1\n",
      "blue = 1\n",
      "[[12. 11. 10.  9.  8.  7.  6.  7.]\n",
      " [11. 10.  9.  8.  7.  6.  5.  6.]\n",
      " [10.  9.  8.  7.  6.  5.  4.  5.]\n",
      " [ 9.  8.  7.  6.  5.  4.  3.  4.]\n",
      " [ 8.  7.  6.  5.  4.  3.  2.  3.]\n",
      " [ 7.  6.  5.  4.  3.  2.  1.  2.]\n",
      " [ 6.  5.  4.  3.  2.  1.  0.  1.]\n",
      " [ 7.  6.  5.  4.  3.  2.  1.  2.]]\n",
      "\n",
      "\n",
      "red = 0\n",
      "blue = 0\n",
      "[[19.69 18.73 17.77 16.82 16.77 17.21 18.62 18.62]\n",
      " [20.63 19.69 18.76 17.83 16.87 17.87 19.68 19.96]\n",
      " [21.6  20.66 19.73 18.78 17.84 19.09 20.89 21.43]\n",
      " [22.57 21.63 20.69 19.73 18.81 20.32 22.09 22.91]\n",
      " [23.54 22.6  21.64 20.68 19.78 21.55 23.3  24.38]\n",
      " [24.51 23.55 22.59 21.63 20.75 22.77 24.51 25.85]\n",
      " [25.52 24.66 23.81 22.95 22.07 24.42 27.39 28.72]\n",
      " [27.55 26.69 25.83 24.81 23.62 25.91 28.76 32.23]]\n",
      "red = 0\n",
      "blue = 1\n",
      "[[ 11.85  12.8   13.79  14.78  15.77  16.75  17.39  17.12]\n",
      " [ 10.86  11.81  12.8   13.78  14.77  15.76  16.31  15.46]\n",
      " [  9.87  10.81  11.8   12.79  13.78  14.77  15.32   7.59]\n",
      " [  8.88   9.82  10.81  11.8   12.79  13.78  14.05  -0.27]\n",
      " [  8.63   8.9    9.89  10.87  11.86  12.7    6.59  -8.1 ]\n",
      " [  9.59   9.96  10.81  11.84  12.86  13.34   1.88 -12.62]\n",
      " [ 10.88  11.26  11.79  12.22  12.33   7.72  -3.74 -18.64]\n",
      " [ 11.46  11.53  11.96  12.28  12.16   2.13  -7.36 -22.17]]\n",
      "red = 1\n",
      "blue = 0\n",
      "[[11.81 10.84  9.87  8.9   8.87  9.06 10.46  9.72]\n",
      " [12.79 11.84 10.89  9.94  8.97  9.48 11.53 10.76]\n",
      " [13.77 12.82 11.87 10.9   9.96 10.72 12.45 11.95]\n",
      " [14.76 13.8  12.83 11.86 10.95 11.83 13.37 13.13]\n",
      " [15.74 14.77 13.79 12.82 11.94 12.72 14.29 14.32]\n",
      " [16.7  15.73 14.75 13.78 12.64 13.62 15.21 15.5 ]\n",
      " [17.66 16.69 15.72 14.64 13.28 14.51 16.13 17.  ]\n",
      " [18.62 17.65 16.68 15.18 13.92 15.46 17.21 17.66]]\n",
      "red = 1\n",
      "blue = 1\n",
      "[[11.84 10.85  9.86  8.87  7.87  6.87  5.87  6.43]\n",
      " [10.84  9.85  8.86  7.87  6.87  5.88  4.89  5.74]\n",
      " [ 9.84  8.85  7.86  6.86  5.87  4.88  3.89  4.78]\n",
      " [ 8.84  7.84  6.85  5.86  4.87  3.88  2.89  3.96]\n",
      " [ 7.83  6.84  5.85  4.86  3.87  2.88  1.89  2.97]\n",
      " [ 6.83  5.84  4.85  3.86  2.87  1.87  0.88  1.89]\n",
      " [ 5.89  4.9   3.92  2.93  1.94  0.95 -0.06  1.33]\n",
      " [ 6.77  5.98  5.    3.87  2.92  1.87  0.89  3.85]]\n"
     ]
    }
   ],
   "source": [
    "print_space(distance, end_state)\n",
    "print('\\n')\n",
    "print_space(predicted_distance, end_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/openai/lib/python3.6/site-packages/ipykernel/__main__.py:19: RuntimeWarning: invalid value encountered in less\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "red = 0\n",
      "blue = 0\n",
      "[[ 0.   -0.   -0.02 -0.03  0.66 -0.26 -0.   -0.  ]\n",
      " [-0.2  -0.09 -0.2  -0.14 -0.01  0.   -0.01 -0.  ]\n",
      " [-0.09 -0.09 -0.08 -0.16 -0.   -0.03 -0.01 -0.  ]\n",
      " [-1.98 -0.1  -0.12 -0.19 -0.01 -0.   -0.02 -0.  ]\n",
      " [-0.   -1.99 -0.12 -0.09 -0.   -0.04 -0.04 -0.  ]\n",
      " [-1.06 -0.08 -0.11 -0.06 -0.01 -0.09 -1.94 -1.  ]\n",
      " [-1.27 -1.46 -0.   -0.   -0.01 -2.   -2.   -1.  ]\n",
      " [-1.18 -1.48 -0.   -0.   -0.   -2.   -2.   -0.  ]]\n",
      "red = 0\n",
      "blue = 1\n",
      "[[-0.01 -0.01 -0.08 -0.03 -0.11 -0.01 -0.06  0.  ]\n",
      " [-0.01 -0.02 -0.06 -0.04  0.   -0.08 -0.02  0.  ]\n",
      " [ 0.02 -0.03 -0.06 -0.11 -0.03 -0.11 -0.1  -0.01]\n",
      " [-0.03 -0.   -0.07 -0.   -0.08 -0.06 -0.02 -0.01]\n",
      " [ 0.91 -0.01 -0.   -0.   -0.   -0.   -0.   -0.5 ]\n",
      " [-0.14 -0.2  -0.15 -0.07 -0.02 -0.03 -0.51 -0.51]\n",
      " [-0.   -0.   -0.   -0.   -0.   -0.03 -0.97 -0.95]\n",
      " [-0.   -0.   -0.   -0.   -0.   -1.14 -0.99 -0.  ]]\n",
      "red = 1\n",
      "blue = 0\n",
      "[[-0.    0.01 -0.02 -0.03  0.49 -0.47 -0.   -0.  ]\n",
      " [-0.14 -0.15 -0.16 -0.07 -0.   -0.04 -0.01 -0.  ]\n",
      " [-0.01 -0.15 -0.14 -0.05 -0.   -0.01 -0.   -0.  ]\n",
      " [-0.12 -0.18 -0.1  -0.08 -0.   -0.04 -0.   -0.  ]\n",
      " [-0.13 -0.08 -0.2  -0.08 -0.   -0.01 -0.02 -0.03]\n",
      " [-0.11 -0.14 -0.13 -0.07 -0.   -0.04 -0.16 -0.52]\n",
      " [-0.07 -0.16 -0.08 -0.08 -0.01 -0.22 -1.55 -0.48]\n",
      " [-0.13 -0.15 -0.24 -0.24 -0.04 -1.77 -1.8  -0.4 ]]\n",
      "red = 1\n",
      "blue = 1\n",
      "[[-0.   -0.1  -0.01 -0.03 -0.16 -0.07 -0.04 -0.03]\n",
      " [-0.02 -0.02 -0.05 -0.12 -0.11 -0.16 -0.03 -0.01]\n",
      " [-0.07 -0.05 -0.06 -0.06 -0.03 -0.11 -0.   -0.02]\n",
      " [-0.08 -0.06 -0.04 -0.1  -0.18 -0.06 -0.   -0.01]\n",
      " [-0.   -0.08 -0.03 -0.03 -0.13 -0.01 -0.   -0.  ]\n",
      " [-0.07 -0.01 -0.01 -0.   -0.14 -0.08 -0.03 -0.03]\n",
      " [-0.   -0.01 -0.   -0.   -0.   -0.01 -0.    0.02]\n",
      " [-0.    0.   -0.   -0.   -0.48 -0.52 -0.01  0.01]]\n"
     ]
    }
   ],
   "source": [
    "print_space(forward_curvature, end_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:openai]",
   "language": "python",
   "name": "conda-env-openai-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
