{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple\n",
    "from itertools import count\n",
    "from PIL import Image\n",
    "from pyemd import emd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "\n",
    "# set up matplotlib\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "# if gpu is to be used\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Saves a transition.\"\"\"\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.position] = Transition(*args)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "def combine(state, goal):\n",
    "    if state is None:\n",
    "        return None\n",
    "    return torch.cat((state, goal), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class miniDQN(nn.Module):\n",
    "\n",
    "    def __init__(self, hidden):\n",
    "        super(miniDQN, self).__init__()\n",
    "        self.W1 = nn.Linear(8, hidden) # state + goal\n",
    "        self.W2 = nn.Linear(hidden, 4)\n",
    "        nn.init.xavier_uniform_(self.W1.weight)\n",
    "        nn.init.xavier_uniform_(self.W2.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.W1(x))\n",
    "        return self.W2(x)\n",
    "\n",
    "#For now, embedding on states (not state-action pairs) because we're lazy\n",
    "class embeddingNN(nn.Module):\n",
    "\n",
    "    def __init__(self, hidden, output):\n",
    "        super(embeddingNN, self).__init__()\n",
    "        self.W1 = nn.Linear(8, hidden) # state + goal\n",
    "        self.W2 = nn.Linear(hidden, output)\n",
    "        nn.init.xavier_uniform_(self.W1.weight)\n",
    "        nn.init.xavier_uniform_(self.W2.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.W1(x))\n",
    "        return self.W2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_row = 10\n",
    "num_col = 10\n",
    "\n",
    "action_direction = [(0,1), (0,-1), (1,0), (-1,0)]\n",
    "\n",
    "reds = [(3,0)]\n",
    "blues = [(0,3)]\n",
    "walls = []\n",
    "\n",
    "start_state = np.asarray([0,0,0,0])\n",
    "end_state = np.asarray([4,4,1,1])\n",
    "torch_end = torch.from_numpy(end_state).float().unsqueeze(0)\n",
    "\n",
    "\n",
    "def is_done(s):\n",
    "    return np.array_equal(s, end_state)\n",
    "\n",
    "def step(s, a):\n",
    "    row, col, blue, red = tuple(s)\n",
    "    a_row, a_col = action_direction[a]\n",
    "\n",
    "    row = row + a_row\n",
    "    col = col + a_col\n",
    "\n",
    "    if row < 0 or row >= num_row or col < 0 or col >= num_col or (row, col) in walls:\n",
    "        return s\n",
    "\n",
    "    blue = blue or (row, col) in blues\n",
    "    red = red or (row, col) in reds\n",
    "\n",
    "    return np.asarray((row, col, blue, red))\n",
    "\n",
    "def get_reward(s, a, s_prime):\n",
    "    if is_done(s):\n",
    "        return 0.0\n",
    "    return -1.0\n",
    "\n",
    "def L1(s1, s2):\n",
    "    return 1.0 * np.sum(np.abs(np.asarray(s1) - np.asarray(s2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Very adhoc, requires no dynamics\n",
    "distance_cache = {}\n",
    "\n",
    "def distance(a, b):\n",
    "    global distance_cache\n",
    "    ta = tuple(a)\n",
    "    tb = tuple(b)\n",
    "    if (ta,tb) in distance_cache:\n",
    "        return distance_cache[(ta,tb)]\n",
    "    \n",
    "    row_a, col_a, blue_a, red_a = ta\n",
    "    row_b, col_b, blue_b, red_b = tb\n",
    "    \n",
    "    blue_match = blue_a == blue_b\n",
    "    red_match = red_a == red_b\n",
    "    \n",
    "    if red_match and not blue_match:\n",
    "        dist = min([L1((row_a,col_a),blue) + L1(blue, (row_b, col_b)) for blue in blues])\n",
    "    elif not red_match and blue_match:\n",
    "        dist = min([L1((row_a,col_a),red) + L1(red, (row_b, col_b)) for red in reds])\n",
    "    elif not red_match and not blue_match:\n",
    "        x = min([L1((row_a,col_a),red) + L1(red, blue) + L1(blue, (row_b, col_b)) for red, blue in zip(reds, blues)])\n",
    "        y = min([L1((row_a,col_a),blue) + L1(blue, red) + L1(red, (row_b, col_b)) for red, blue in zip(reds, blues)])\n",
    "        dist = min(x,y)\n",
    "    else:\n",
    "        dist = L1(ta, tb)\n",
    "    distance_cache[(ta,tb)] = dist\n",
    "    return dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "GAMMA = 0.9999\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 6000\n",
    "TARGET_UPDATE = 50\n",
    "HIDDEN_LAYER_SIZE = 15\n",
    "EMBEDDING_SIZE = 6\n",
    "EMBEDDING_LAMBDA = 0.01\n",
    "EPISODE_LENGTH = 30\n",
    "LEARNING_RATE = 0.0004\n",
    "\n",
    "DIVERSE_EXPERT = True\n",
    "\n",
    "num_episodes = 30000\n",
    "random.seed(10)\n",
    "distance_cache = {}\n",
    "\n",
    "policy_net = miniDQN(HIDDEN_LAYER_SIZE).to(device)\n",
    "target_net = miniDQN(HIDDEN_LAYER_SIZE).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "\n",
    "optimizer = optim.Adam(policy_net.parameters(), lr = LEARNING_RATE)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, 0.99999)\n",
    "memory = ReplayMemory(40000)\n",
    "expert_memory = ReplayMemory(40000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_net = embeddingNN(HIDDEN_LAYER_SIZE, EMBEDDING_SIZE).to(device)\n",
    "\n",
    "embedding_optimizer = optim.Adam(embedding_net.parameters(), lr = LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_done = 0\n",
    "\n",
    "#Epsilon greedy actions\n",
    "def select_action(state, goal, test = False):\n",
    "    global steps_done\n",
    "    sample = random.random()\n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * math.exp(-1. * steps_done / EPS_DECAY)\n",
    "    steps_done += 1\n",
    "    if test or sample > eps_threshold:\n",
    "        with torch.no_grad():\n",
    "            x = torch.cat((state, goal), 1)\n",
    "            return policy_net(x).max(1)[1].view(1, 1)\n",
    "    else:\n",
    "        return torch.tensor([[random.randrange(4)]], device=device, dtype=torch.long)\n",
    "\n",
    "#Max entropy action\n",
    "def select_action_entropy(state, goal, alpha):\n",
    "    x = torch.cat((state, goal), 1)\n",
    "    potential = (alpha * policy_net(x).detach()).exp().squeeze()\n",
    "    dist = (potential / potential.sum()).numpy()\n",
    "    return torch.tensor([[np.random.choice(4, p = dist)]], device=device, dtype=torch.long)\n",
    "    \n",
    "\n",
    "def select_expert_action(state, goal):\n",
    "    row, col, blue, red = tuple(state)\n",
    "    action_scores = np.zeros(4)\n",
    "    for a in range(4):\n",
    "        #Assumes determinism\n",
    "        next_state = step(state, a)\n",
    "        action_scores[a] = distance(next_state, goal)\n",
    "    action = np.random.choice(np.flatnonzero(action_scores == action_scores.min()))\n",
    "    return torch.tensor([[action]], device=device, dtype=torch.long)    \n",
    "\n",
    "def sample_expert_trajectory():\n",
    "    state = start_state\n",
    "    next_state = None\n",
    "\n",
    "    trajectory = []\n",
    "    \n",
    "    for t in range(EPISODE_LENGTH):\n",
    "        action = select_expert_action(state, end_state)\n",
    "        \n",
    "        torch_state = torch.from_numpy(state).float().unsqueeze(0)\n",
    "        next_state = step(state, action.item())\n",
    "        torch_next_state = torch.from_numpy(next_state).float().unsqueeze(0)\n",
    "        reward = get_reward(state, action.item(), next_state)\n",
    "        done = is_done(state)\n",
    "        reward = torch.tensor([reward], device=device)\n",
    "        if done:\n",
    "            torch_next_state = None\n",
    "\n",
    "        trajectory.append((torch_state, action, torch_next_state, reward))\n",
    "        if done:\n",
    "            break\n",
    "        state = next_state\n",
    "    return trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model():\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "    #randomly learn over expert trajectories or rollout buffer\n",
    "    transitions = memory.sample(BATCH_SIZE) if np.random.random() > 0.5 else expert_memory.sample(BATCH_SIZE)\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,batch.next_state)),\n",
    "                                  device=device, dtype=torch.uint8)\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state if s is not None])\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
    "\n",
    "    #DDQN\n",
    "    policy_action_indices = policy_net(non_final_next_states).max(1)[1].detach()\n",
    "    target_actions = target_net(non_final_next_states).detach()\n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "    next_state_values[non_final_mask] = target_actions.gather(1, policy_action_indices.unsqueeze(1)).squeeze()\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "    loss = F.smooth_l1_loss(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "    \n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    for param in policy_net.parameters():\n",
    "        param.grad.data.clamp_(-1, 1)\n",
    "    optimizer.step()\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_buffer():\n",
    "    transitions = memory.sample(BATCH_SIZE) if np.random.random() > 0.5 else expert_memory.sample(BATCH_SIZE)\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,batch.next_state)),\n",
    "                                  device=device, dtype=torch.uint8)\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state if s is not None])\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "    \n",
    "    state_embeddings = embedding_net(state_batch)\n",
    "    next_state_embeddings = torch.zeros((BATCH_SIZE, EMBEDDING_SIZE), device=device)\n",
    "    next_state_embeddings[non_final_mask,:] = embedding_net(non_final_next_states)\n",
    "        \n",
    "    return (state_embeddings, reward_batch, next_state_embeddings)\n",
    "\n",
    "\n",
    "def optimize_bisim():\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "    \n",
    "    sample_1 = sample_buffer()\n",
    "    sample_2 = sample_buffer()\n",
    "\n",
    "    distance = torch.norm(sample_1[0] - sample_2[0], dim = 1)\n",
    "    reward_distance = torch.abs(sample_1[1] - sample_2[1])\n",
    "    next_distance = torch.norm(sample_1[2] - sample_2[2], dim = 1)\n",
    "    \n",
    "    TD_error = reward_distance + GAMMA * next_distance\n",
    "    \n",
    "    loss = F.smooth_l1_loss(distance, TD_error.detach()) #Detaching because the loss function complains\n",
    "    \n",
    "    loss += EMBEDDING_LAMBDA * torch.mean(torch.norm(sample_1[0], dim = 1) + torch.norm(sample_2[0], dim = 1))\n",
    "    \n",
    "    # Optimize the model\n",
    "    embedding_optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    for param in policy_net.parameters():\n",
    "        param.grad.data.clamp_(-1, 1)\n",
    "    embedding_optimizer.step()\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = [0]\n",
    "expert_trajectory = sample_expert_trajectory()\n",
    "#pre-populate the replay buffers\n",
    "for i in range(1000):\n",
    "    if DIVERSE_EXPERT:\n",
    "        expert_trajectory = sample_expert_trajectory()\n",
    "    for torch_state, action, torch_next_state, reward in expert_trajectory:\n",
    "        memory.push(combine(torch_state, torch_end), action, combine(torch_next_state, torch_end), reward)\n",
    "        expert_memory.push(combine(torch_state, torch_end), action, combine(torch_next_state, torch_end), reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0.6312915288632915\n",
      "1000\n",
      "0.08762718802983208\n",
      "2000\n",
      "0.1404543710306425\n",
      "3000\n",
      "0.17711578013387436\n",
      "4000\n",
      "0.10978186437314605\n",
      "5000\n",
      "0.050820254418993974\n",
      "6000\n",
      "0.03119435774156541\n",
      "7000\n",
      "0.022082727984731713\n",
      "8000\n",
      "0.015939105472129397\n",
      "9000\n",
      "0.014727433244492917\n",
      "10000\n",
      "0.0124022966361009\n",
      "11000\n",
      "0.008364319186442044\n",
      "12000\n",
      "0.006286344531851153\n",
      "13000\n",
      "0.011872495687833655\n",
      "14000\n",
      "0.014769212168452818\n",
      "15000\n",
      "0.013327201313902363\n",
      "16000\n",
      "0.014145487907746774\n",
      "17000\n",
      "0.016953523750812802\n",
      "18000\n",
      "0.011899025037636176\n",
      "19000\n",
      "0.006097320523448108\n",
      "20000\n",
      "0.0051427513863071175\n",
      "21000\n",
      "0.005531684914489484\n",
      "22000\n",
      "0.004880143143681759\n",
      "23000\n",
      "0.004637071082103994\n",
      "24000\n",
      "0.0047074228836770215\n",
      "25000\n",
      "0.005003519789869507\n",
      "26000\n",
      "0.0045951351621715646\n",
      "27000\n",
      "0.00456770157946877\n",
      "28000\n",
      "0.004796006188372141\n",
      "29000\n",
      "0.004373626625805216\n",
      "[0 0 0 0]\n",
      "[1 0 0 0]\n",
      "[2 0 0 0]\n",
      "[3 0 0 1]\n",
      "[2 0 0 1]\n",
      "[2 1 0 1]\n",
      "[2 2 0 1]\n",
      "[1 2 0 1]\n",
      "[1 3 0 1]\n",
      "[0 3 1 1]\n",
      "[1 3 1 1]\n",
      "[1 4 1 1]\n",
      "[2 4 1 1]\n",
      "[3 4 1 1]\n",
      "[4 4 1 1]\n",
      "Complete\n"
     ]
    }
   ],
   "source": [
    "for i_episode in range(num_episodes):\n",
    "    state = start_state\n",
    "    next_state = None\n",
    "    final = i_episode == num_episodes - 1\n",
    "    trajectory = []\n",
    "    \n",
    "    if i_episode % 1000 == 0:\n",
    "        scheduler.step()\n",
    "    \n",
    "    for t in range(EPISODE_LENGTH):\n",
    "        torch_state = torch.from_numpy(state).float().unsqueeze(0)\n",
    "\n",
    "        action = select_action(torch_state, torch_end, final)\n",
    "            \n",
    "        next_state = step(state, action.item())\n",
    "        torch_next_state = torch.from_numpy(next_state).float().unsqueeze(0)\n",
    "        reward = get_reward(state, action.item(), next_state)\n",
    "        done = is_done(state)\n",
    "        reward = torch.tensor([reward], device=device)\n",
    "\n",
    "        if done:\n",
    "            torch_next_state = None\n",
    "\n",
    "        trajectory.append((torch_state, action, torch_next_state, reward))\n",
    "\n",
    "        loss = optimize_model()\n",
    "        optimize_bisim()\n",
    "        losses.append(loss if loss is not None else 0)\n",
    "        if final:\n",
    "            print(state)\n",
    "        if done:\n",
    "            break\n",
    "        state = next_state\n",
    "\n",
    "    if i_episode % 1000 == 0:\n",
    "        print(i_episode)\n",
    "        print(np.mean(np.asarray(losses)))\n",
    "        losses = []\n",
    "\n",
    "    #HER updates with goal as real goal OR last state in trajectory\n",
    "    for torch_state, action, torch_next_state, reward in trajectory:\n",
    "        memory.push(combine(torch_state, torch_end), action, combine(torch_next_state, torch_end), reward)\n",
    "        if not is_done(state):\n",
    "            torch_local_goal = torch.from_numpy(state).float().unsqueeze(0)\n",
    "            local_reward = torch.equal(torch_next_state, torch_local_goal)\n",
    "            new_reward = torch.add(reward, local_reward)\n",
    "            memory.push(combine(torch_state, torch_local_goal), action, combine(torch_next_state, torch_local_goal), new_reward)\n",
    "                    \n",
    "                          \n",
    "    # Update the target network\n",
    "    if i_episode % TARGET_UPDATE == 0:\n",
    "        target_net.load_state_dict(policy_net.state_dict())\n",
    "    \n",
    "        \n",
    "print('Complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1647, -0.4212, -0.1037,  0.0042,  0.5669, -0.1319]],\n",
      "       grad_fn=<ThAddmmBackward>)\n",
      "tensor([[-0.1850, -0.6927,  0.0027,  0.0071,  0.2250, -0.1876]],\n",
      "       grad_fn=<ThAddmmBackward>)\n",
      "tensor([[-0.6710, -0.4280, -0.0702,  0.4902,  0.0672,  0.4747]],\n",
      "       grad_fn=<ThAddmmBackward>)\n",
      "tensor([[ 0.1164,  0.1212,  0.0615, -0.4809,  0.1225, -0.3981]],\n",
      "       grad_fn=<ThAddmmBackward>)\n",
      "tensor([[-0.6487, -0.0256,  0.1402, -0.2351, -0.5352,  0.1818]],\n",
      "       grad_fn=<ThAddmmBackward>)\n",
      "tensor([[-0.6080,  0.5175, -0.0726, -0.2409,  0.1488,  0.2931]],\n",
      "       grad_fn=<ThAddmmBackward>)\n",
      "tensor([[-1.1255,  0.2376,  0.0177, -0.7216, -0.1673,  0.2399]],\n",
      "       grad_fn=<ThAddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "def predicted_embedding(state, goal):\n",
    "    torch_state = torch.from_numpy(state).float().unsqueeze(0)\n",
    "    torch_goal = torch.from_numpy(goal).float().unsqueeze(0)\n",
    "    x = torch.cat((torch_state, torch_goal), 1)\n",
    "    return embedding_net(x)\n",
    "\n",
    "print(predicted_embedding(np.asarray([2,1,0,0]), end_state))\n",
    "print(predicted_embedding(np.asarray([1,2,0,0]), end_state))\n",
    "print(predicted_embedding(np.asarray([2,2,0,1]), end_state))\n",
    "print(predicted_embedding(np.asarray([2,2,1,0]), end_state))\n",
    "print(predicted_embedding(np.asarray([2,4,1,1]), end_state))\n",
    "print(predicted_embedding(np.asarray([4,2,1,1]), end_state))\n",
    "print(predicted_embedding(np.asarray([4,4,1,1]), end_state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predicted_distance(state, goal):\n",
    "    torch_state = torch.from_numpy(state).float().unsqueeze(0)\n",
    "    torch_goal = torch.from_numpy(goal).float().unsqueeze(0)\n",
    "    x = torch.cat((torch_state, torch_goal), 1)\n",
    "    return -1 * policy_net(x).max(1)[0].view(1, 1).item()\n",
    "\n",
    "def predicted_measure_ball(state, goal, n = 500):\n",
    "    measure = {}\n",
    "    torch_state = torch.from_numpy(state).float().unsqueeze(0)\n",
    "    torch_goal = torch.from_numpy(goal).float().unsqueeze(0)\n",
    "    x = torch.cat((torch_state, torch_goal), 1)\n",
    "    for i in range(n):\n",
    "        action = select_action_entropy(torch_state, torch_goal, alpha = 4.0)\n",
    "        next_state = step(state, action.item())\n",
    "        key = tuple(next_state)\n",
    "        measure[key] = measure.get(key, 0) + 1.0/n\n",
    "    return measure\n",
    "\n",
    "def wasserstein(a, b, goal):\n",
    "    if tuple(a) == tuple(b):\n",
    "        return 0\n",
    "    m_a = predicted_measure_ball(a, goal)\n",
    "    m_b = predicted_measure_ball(b, goal)\n",
    "    states = list(set(m_a.keys()) | set(m_b.keys()))\n",
    "    \n",
    "    m_a = [m_a.get(s, 0.0) for s in states]\n",
    "    m_b = [m_b.get(s, 0.0) for s in states]\n",
    "    dist = [[distance(np.asarray(s1),np.asarray(s2)) for s1 in states] for s2 in states]\n",
    "    return emd(np.asarray(m_a), np.asarray(m_b), np.asarray(dist))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[13.8  12.84 11.92 11.16  9.98 11.21 12.44 12.75 12.04 11.34]\n",
      " [13.05 13.6  12.99 11.23 11.33 12.57 13.8  13.47 12.76 12.06]\n",
      " [12.07 14.29 13.79 12.7  12.69 13.92 14.9  14.19 13.49 12.78]\n",
      " [13.91 15.14 15.26 14.18 14.05 15.28 15.62 14.92 14.21 13.51]\n",
      " [16.52 17.22 17.78 16.52 16.11 17.17 16.7  15.83 14.95 14.23]\n",
      " [18.31 19.28 20.25 19.48 18.95 19.79 18.91 18.03 17.15 16.28]\n",
      " [19.7  20.67 21.64 21.78 21.79 21.99 21.12 20.24 19.36 18.48]\n",
      " [21.09 22.06 23.03 23.77 23.63 24.2  23.32 22.45 21.57 20.69]\n",
      " [22.48 23.45 24.42 25.4  25.22 25.61 25.53 24.65 23.77 22.9 ]\n",
      " [23.87 24.84 25.81 26.76 26.14 26.43 26.72 26.46 25.49 24.52]]\n",
      "[[ 8.03  9.1  10.03 10.96 10.27 10.56 11.64 12.72 13.8  13.4 ]\n",
      " [ 7.21  8.14  9.07  9.95 10.2   9.79 10.88 11.96 12.87 12.01]\n",
      " [ 6.24  7.17  8.1   8.94  9.4   8.78  9.97 11.44 12.1  11.63]\n",
      " [ 5.62  6.19  7.18  8.11  9.04  9.06 10.47 11.93 11.96 11.49]\n",
      " [ 6.35  7.72  8.65  9.59 10.52 11.04 12.34 13.5  12.85 12.21]\n",
      " [ 7.08  8.29  9.5  10.71 11.92 13.02 14.31 14.84 14.2  13.56]\n",
      " [ 7.59  8.8  10.01 11.22 12.43 14.05 16.29 16.19 15.54 14.9 ]\n",
      " [ 8.11  9.32 10.53 11.74 12.95 14.38 18.15 17.53 16.89 16.25]\n",
      " [ 8.62  9.83 11.04 12.25 13.46 14.71 18.31 18.88 18.23 17.59]\n",
      " [ 9.13 10.34 11.55 12.76 13.98 15.19 18.26 19.1  19.58 18.94]]\n",
      "[[ 8.06  7.08  6.21  5.93  8.66  9.18  8.    6.82  5.77  5.07]\n",
      " [ 9.01  8.14  7.16  6.11  8.8   8.61  7.9   7.2   6.46  5.66]\n",
      " [10.04  9.07  8.06  7.29  9.41  8.61  7.82  7.02  6.22  5.42]\n",
      " [11.04 10.03  9.03  8.08  9.17  8.38  7.58  6.78  5.98  5.18]\n",
      " [12.01 11.    9.99  8.99  8.93  8.14  7.34  6.54  5.74  4.94]\n",
      " [13.   11.99 10.98  9.87  9.15  8.18  7.21  6.3   5.5   4.7 ]\n",
      " [14.01 13.   11.99 10.11  9.92  9.42  8.45  7.48  6.51  5.54]\n",
      " [15.01 14.   12.99 10.35  9.98 10.27  9.7   8.73  7.75  6.78]\n",
      " [16.02 15.01 13.95 10.59 10.04 10.33 10.63  9.97  9.    8.03]\n",
      " [17.02 16.01 14.19 10.84 10.1  10.39 10.69 10.98 10.24  9.27]]\n",
      "[[11.53  7.55  6.2   5.14  4.11  5.33  6.55  6.52  5.93  5.34]\n",
      " [ 8.12  6.18  5.18  4.13  3.07  4.2   5.39  4.8   4.21  3.62]\n",
      " [ 6.15  5.15  4.14  3.11  2.06  3.07  3.67  3.08  2.44  1.49]\n",
      " [ 5.12  4.11  3.11  2.1   1.04  1.95  1.95  1.03  0.08 -0.72]\n",
      " [ 4.19  3.16  2.13  1.09  0.02  1.02  0.55  0.08 -0.39 -0.86]\n",
      " [ 3.53  3.09  2.5   1.82  1.14  2.19  1.54  0.9   0.26 -0.38]\n",
      " [ 4.23  4.29  3.61  2.93  2.25  2.64  2.89  2.25  1.6   0.96]\n",
      " [ 4.75  5.4   4.72  4.04  3.36  2.81  3.43  3.59  2.95  2.31]\n",
      " [ 5.26  6.47  5.82  5.15  3.75  2.97  3.59  4.22  4.29  3.62]\n",
      " [ 5.77  6.98  6.93  6.25  4.1   3.13  3.76  4.38  5.01  4.86]]\n"
     ]
    }
   ],
   "source": [
    "values = np.zeros((num_row, num_col, 2, 2))\n",
    "for index in np.ndindex(num_row, num_col, 2, 2):\n",
    "    values[index] = predicted_distance(np.asarray(index), end_state)\n",
    "print(np.round(values[:,:,0,0], 2))\n",
    "print(np.round(values[:,:,1,0], 2))\n",
    "print(np.round(values[:,:,0,1], 2))\n",
    "print(np.round(values[:,:,1,1], 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{(1, 1, 1, 1): 0.6840000000000005, (0, 2, 1, 1): 0.3140000000000002, (0, 1, 1, 1): 0.002}\n",
      "{(1, 1, 1, 1): 0.25400000000000017, (2, 0, 1, 1): 0.7460000000000006}\n"
     ]
    }
   ],
   "source": [
    "def reflect_state(state):\n",
    "    row, col, blue, red = tuple(state)\n",
    "    return np.asarray((col, row, red, blue))\n",
    "\n",
    "def compare(state):\n",
    "    reflected = reflect_state(state)\n",
    "    m_a = predicted_measure_ball(state, end_state)\n",
    "    m_b = predicted_measure_ball(reflected, end_state)\n",
    "    print(m_a)\n",
    "    print(m_b)\n",
    "\n",
    "compare(np.asarray((0,1,1,1)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:openai]",
   "language": "python",
   "name": "conda-env-openai-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
